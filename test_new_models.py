#!/usr/bin/env python3
"""
Test the model-specific configuration for the new models.
"""

import logging
logging.basicConfig(level=logging.INFO)

from job_board_aggregator.api.cerebras.cerebras_validator import CerebrasSchemaValidator

def main():
    print("=== NEW MODEL CONFIGURATION TEST ===")
    
    validator = CerebrasSchemaValidator()
    
    # Test all models to see their mode assignment
    print(f"Available models: {len(validator.available_models)}")
    print(f"Schema mode override models: {validator.schema_mode_models}")
    
    print("\n=== MODEL MODE ASSIGNMENTS ===")
    for model in validator.available_models:
        uses_schema = model.name in validator.schema_mode_models
        mode = "ğŸ”§ Schema mode" if uses_schema else "ğŸ“ JSON mode"
        status = "(FIXED)" if uses_schema and "thinking" in model.name else ""
        print(f"{mode} {model.display_name} ({model.name}) {status}")
    
    print("\n=== PROBLEM RESOLUTION ===")
    print("âœ… Qwen 3 235B Thinking added to schema mode override")
    print("âœ… Will use strict schema enforcement instead of JSON mode")
    print("âœ… Should prevent 400 Bad Request errors from verbosity")
    print("âœ… Other models remain in JSON mode for optimal performance")
    
    print("\n=== EXPECTED BEHAVIOR IMPROVEMENT ===")
    print("ğŸ”§ Qwen 3 235B Thinking: Schema mode â†’ More structured, less verbose")
    print("ğŸ“ GPT OSS 120B: JSON mode â†’ Fast and flexible (monitor for issues)")
    print("ğŸ“ Other models: Continue working as before")
    
    print("\nğŸ‰ System optimized for new model characteristics!")

if __name__ == "__main__":
    main()